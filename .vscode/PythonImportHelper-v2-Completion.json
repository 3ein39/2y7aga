[
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "jsonify",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "render_template",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "predict_all",
        "importPath": "audio",
        "description": "audio",
        "isExtraImport": true,
        "detail": "audio",
        "documentation": {}
    },
    {
        "label": "CORS",
        "importPath": "flask_cors",
        "description": "flask_cors",
        "isExtraImport": true,
        "detail": "flask_cors",
        "documentation": {}
    },
    {
        "label": "AutoFeatureExtractor",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "WhisperForAudioClassification",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "librosa",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "librosa",
        "description": "librosa",
        "detail": "librosa",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 2,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "def index():\n    return render_template('index.html') \n@app.route('/upload-audio', methods=['POST'])\ndef upload_audio():\n    if 'audio' not in request.files:\n        return \"No audio part\", 400\n    file = request.files['audio']\n    if file.filename == '':\n        return \"No selected file\", 400\n    if file:",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "upload_audio",
        "kind": 2,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "def upload_audio():\n    if 'audio' not in request.files:\n        return \"No audio part\", 400\n    file = request.files['audio']\n    if file.filename == '':\n        return \"No selected file\", 400\n    if file:\n        # You can add file saving logic here\n        filename = f'uploads/{file.filename}'\n        file.save(filename)",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "app = Flask(__name__)\nCORS(app)\n@app.route('/')\ndef index():\n    return render_template('index.html') \n@app.route('/upload-audio', methods=['POST'])\ndef upload_audio():\n    if 'audio' not in request.files:\n        return \"No audio part\", 400\n    file = request.files['audio']",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "load_audio_from_path",
        "kind": 2,
        "importPath": "audio",
        "description": "audio",
        "peekOfCode": "def load_audio_from_path(file_path, feature_extractor, max_length=MAX_LENGTH):\n    audio, _ = librosa.load(file_path, sr=SAMPLEING_RATE)\n    audio_length = len(audio)\n    # Splitting the audio if it's longer than max_length\n    segments = []\n    for start in range(0, audio_length, max_length):\n        end = min(start + max_length, audio_length)\n        segment = audio[start:end]\n        inputs = feature_extractor(segment, sampling_rate=SAMPLEING_RATE, return_tensors=\"pt\", max_length=max_length, padding=\"max_length\", ).input_features\n        segments.append(inputs)",
        "detail": "audio",
        "documentation": {}
    },
    {
        "label": "model_generate",
        "kind": 2,
        "importPath": "audio",
        "description": "audio",
        "peekOfCode": "def model_generate(inputs, model):\n    logits = model(inputs.to(device))[0]\n    return logits\ndef postprocess(logits, model, top_k=1):\n    scores = logits[0].softmax(-1).cpu()\n    ids = torch.argmax(scores, dim=-1).item()\n    scores = scores.tolist()\n    labels = model.config.id2label[ids]\n    return labels, round(scores[ids], 2)\ndef predict(segments, model):",
        "detail": "audio",
        "documentation": {}
    },
    {
        "label": "postprocess",
        "kind": 2,
        "importPath": "audio",
        "description": "audio",
        "peekOfCode": "def postprocess(logits, model, top_k=1):\n    scores = logits[0].softmax(-1).cpu()\n    ids = torch.argmax(scores, dim=-1).item()\n    scores = scores.tolist()\n    labels = model.config.id2label[ids]\n    return labels, round(scores[ids], 2)\ndef predict(segments, model):\n    all_logits = []\n    for segment in segments:\n        logits = model_generate(segment, model)",
        "detail": "audio",
        "documentation": {}
    },
    {
        "label": "predict",
        "kind": 2,
        "importPath": "audio",
        "description": "audio",
        "peekOfCode": "def predict(segments, model):\n    all_logits = []\n    for segment in segments:\n        logits = model_generate(segment, model)\n        all_logits.append(logits)\n    # Aggregating the results (simple average)\n    avg_logits = torch.mean(torch.stack(all_logits), dim=0)\n    return postprocess(avg_logits, model)\ndef prdict_accuracy(file_path):\n    result = predict(file_path, acc_model)",
        "detail": "audio",
        "documentation": {}
    },
    {
        "label": "prdict_accuracy",
        "kind": 2,
        "importPath": "audio",
        "description": "audio",
        "peekOfCode": "def prdict_accuracy(file_path):\n    result = predict(file_path, acc_model)\n    return result\ndef predict_fluency(file_path):\n    result = predict(file_path, fluency_model)\n    return result\ndef predict_all(file_path):\n    segments = load_audio_from_path(file_path, acc_feature)\n    acc = predict(segments, acc_model)\n    fle = predict(segments, fluency_model)",
        "detail": "audio",
        "documentation": {}
    },
    {
        "label": "predict_fluency",
        "kind": 2,
        "importPath": "audio",
        "description": "audio",
        "peekOfCode": "def predict_fluency(file_path):\n    result = predict(file_path, fluency_model)\n    return result\ndef predict_all(file_path):\n    segments = load_audio_from_path(file_path, acc_feature)\n    acc = predict(segments, acc_model)\n    fle = predict(segments, fluency_model)\n    return acc, fle\nif __name__ == '__main__':\n    file_path = r'uploads\\audio.wav'",
        "detail": "audio",
        "documentation": {}
    },
    {
        "label": "predict_all",
        "kind": 2,
        "importPath": "audio",
        "description": "audio",
        "peekOfCode": "def predict_all(file_path):\n    segments = load_audio_from_path(file_path, acc_feature)\n    acc = predict(segments, acc_model)\n    fle = predict(segments, fluency_model)\n    return acc, fle\nif __name__ == '__main__':\n    file_path = r'uploads\\audio.wav'\n    print('start')\n    result = predict_fluency(file_path)\n    print('done')",
        "detail": "audio",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "audio",
        "description": "audio",
        "peekOfCode": "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n# device = 'cpu'\nprint('Run on:', device)\nSAMPLEING_RATE = 16000\nMAX_LENGTH = SAMPLEING_RATE * 10 # 10 seconds \nfluency_model_name = \"seba3y/whisper-tiny-fluency\" #future use\nacc_model_name = 'seba3y/whisper-tiny-accuracy'\nfluency_feature = AutoFeatureExtractor.from_pretrained(fluency_model_name)\nfluency_model = WhisperForAudioClassification.from_pretrained(fluency_model_name).to(device)\nacc_feature = AutoFeatureExtractor.from_pretrained(acc_model_name)",
        "detail": "audio",
        "documentation": {}
    },
    {
        "label": "SAMPLEING_RATE",
        "kind": 5,
        "importPath": "audio",
        "description": "audio",
        "peekOfCode": "SAMPLEING_RATE = 16000\nMAX_LENGTH = SAMPLEING_RATE * 10 # 10 seconds \nfluency_model_name = \"seba3y/whisper-tiny-fluency\" #future use\nacc_model_name = 'seba3y/whisper-tiny-accuracy'\nfluency_feature = AutoFeatureExtractor.from_pretrained(fluency_model_name)\nfluency_model = WhisperForAudioClassification.from_pretrained(fluency_model_name).to(device)\nacc_feature = AutoFeatureExtractor.from_pretrained(acc_model_name)\nacc_model = WhisperForAudioClassification.from_pretrained(acc_model_name).to(device)\ndef load_audio_from_path(file_path, feature_extractor, max_length=MAX_LENGTH):\n    audio, _ = librosa.load(file_path, sr=SAMPLEING_RATE)",
        "detail": "audio",
        "documentation": {}
    },
    {
        "label": "MAX_LENGTH",
        "kind": 5,
        "importPath": "audio",
        "description": "audio",
        "peekOfCode": "MAX_LENGTH = SAMPLEING_RATE * 10 # 10 seconds \nfluency_model_name = \"seba3y/whisper-tiny-fluency\" #future use\nacc_model_name = 'seba3y/whisper-tiny-accuracy'\nfluency_feature = AutoFeatureExtractor.from_pretrained(fluency_model_name)\nfluency_model = WhisperForAudioClassification.from_pretrained(fluency_model_name).to(device)\nacc_feature = AutoFeatureExtractor.from_pretrained(acc_model_name)\nacc_model = WhisperForAudioClassification.from_pretrained(acc_model_name).to(device)\ndef load_audio_from_path(file_path, feature_extractor, max_length=MAX_LENGTH):\n    audio, _ = librosa.load(file_path, sr=SAMPLEING_RATE)\n    audio_length = len(audio)",
        "detail": "audio",
        "documentation": {}
    },
    {
        "label": "fluency_model_name",
        "kind": 5,
        "importPath": "audio",
        "description": "audio",
        "peekOfCode": "fluency_model_name = \"seba3y/whisper-tiny-fluency\" #future use\nacc_model_name = 'seba3y/whisper-tiny-accuracy'\nfluency_feature = AutoFeatureExtractor.from_pretrained(fluency_model_name)\nfluency_model = WhisperForAudioClassification.from_pretrained(fluency_model_name).to(device)\nacc_feature = AutoFeatureExtractor.from_pretrained(acc_model_name)\nacc_model = WhisperForAudioClassification.from_pretrained(acc_model_name).to(device)\ndef load_audio_from_path(file_path, feature_extractor, max_length=MAX_LENGTH):\n    audio, _ = librosa.load(file_path, sr=SAMPLEING_RATE)\n    audio_length = len(audio)\n    # Splitting the audio if it's longer than max_length",
        "detail": "audio",
        "documentation": {}
    },
    {
        "label": "acc_model_name",
        "kind": 5,
        "importPath": "audio",
        "description": "audio",
        "peekOfCode": "acc_model_name = 'seba3y/whisper-tiny-accuracy'\nfluency_feature = AutoFeatureExtractor.from_pretrained(fluency_model_name)\nfluency_model = WhisperForAudioClassification.from_pretrained(fluency_model_name).to(device)\nacc_feature = AutoFeatureExtractor.from_pretrained(acc_model_name)\nacc_model = WhisperForAudioClassification.from_pretrained(acc_model_name).to(device)\ndef load_audio_from_path(file_path, feature_extractor, max_length=MAX_LENGTH):\n    audio, _ = librosa.load(file_path, sr=SAMPLEING_RATE)\n    audio_length = len(audio)\n    # Splitting the audio if it's longer than max_length\n    segments = []",
        "detail": "audio",
        "documentation": {}
    },
    {
        "label": "fluency_feature",
        "kind": 5,
        "importPath": "audio",
        "description": "audio",
        "peekOfCode": "fluency_feature = AutoFeatureExtractor.from_pretrained(fluency_model_name)\nfluency_model = WhisperForAudioClassification.from_pretrained(fluency_model_name).to(device)\nacc_feature = AutoFeatureExtractor.from_pretrained(acc_model_name)\nacc_model = WhisperForAudioClassification.from_pretrained(acc_model_name).to(device)\ndef load_audio_from_path(file_path, feature_extractor, max_length=MAX_LENGTH):\n    audio, _ = librosa.load(file_path, sr=SAMPLEING_RATE)\n    audio_length = len(audio)\n    # Splitting the audio if it's longer than max_length\n    segments = []\n    for start in range(0, audio_length, max_length):",
        "detail": "audio",
        "documentation": {}
    },
    {
        "label": "fluency_model",
        "kind": 5,
        "importPath": "audio",
        "description": "audio",
        "peekOfCode": "fluency_model = WhisperForAudioClassification.from_pretrained(fluency_model_name).to(device)\nacc_feature = AutoFeatureExtractor.from_pretrained(acc_model_name)\nacc_model = WhisperForAudioClassification.from_pretrained(acc_model_name).to(device)\ndef load_audio_from_path(file_path, feature_extractor, max_length=MAX_LENGTH):\n    audio, _ = librosa.load(file_path, sr=SAMPLEING_RATE)\n    audio_length = len(audio)\n    # Splitting the audio if it's longer than max_length\n    segments = []\n    for start in range(0, audio_length, max_length):\n        end = min(start + max_length, audio_length)",
        "detail": "audio",
        "documentation": {}
    },
    {
        "label": "acc_feature",
        "kind": 5,
        "importPath": "audio",
        "description": "audio",
        "peekOfCode": "acc_feature = AutoFeatureExtractor.from_pretrained(acc_model_name)\nacc_model = WhisperForAudioClassification.from_pretrained(acc_model_name).to(device)\ndef load_audio_from_path(file_path, feature_extractor, max_length=MAX_LENGTH):\n    audio, _ = librosa.load(file_path, sr=SAMPLEING_RATE)\n    audio_length = len(audio)\n    # Splitting the audio if it's longer than max_length\n    segments = []\n    for start in range(0, audio_length, max_length):\n        end = min(start + max_length, audio_length)\n        segment = audio[start:end]",
        "detail": "audio",
        "documentation": {}
    },
    {
        "label": "acc_model",
        "kind": 5,
        "importPath": "audio",
        "description": "audio",
        "peekOfCode": "acc_model = WhisperForAudioClassification.from_pretrained(acc_model_name).to(device)\ndef load_audio_from_path(file_path, feature_extractor, max_length=MAX_LENGTH):\n    audio, _ = librosa.load(file_path, sr=SAMPLEING_RATE)\n    audio_length = len(audio)\n    # Splitting the audio if it's longer than max_length\n    segments = []\n    for start in range(0, audio_length, max_length):\n        end = min(start + max_length, audio_length)\n        segment = audio[start:end]\n        inputs = feature_extractor(segment, sampling_rate=SAMPLEING_RATE, return_tensors=\"pt\", max_length=max_length, padding=\"max_length\", ).input_features",
        "detail": "audio",
        "documentation": {}
    }
]